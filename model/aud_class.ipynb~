{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exploratory Data Analysis of Audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "#\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "#\n",
    "# import IPython.display as ipd\n",
    "#\n",
    "# filepath = \"model/UrbanSound8K/audio/fold1/7061-6-0-0.wav\"\n",
    "# ipd.Audio(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import librosa\n",
    "# import librosa.display\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# data, sample_rate = librosa.load(filepath)\n",
    "#\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# librosa.display.waveshow(data, sr=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # not recommended\n",
    "# from scipy.io import wavfile as wav\n",
    "#\n",
    "# wave_sample_rate, wave_audio = wav.read(filepath)\n",
    "# print(wave_sample_rate)\n",
    "# print(wave_audio)\n",
    "#\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# plt.plot(wave_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imbalance Dataset check\n",
    "import pandas as pd\n",
    "metadata = pd.read_csv('C:/Users/ashva/Projects/Audio Classifier/model/UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# metadata['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.countplot(metadata['class'])\n",
    "# plt.title(\"Count of records in each class\")\n",
    "# plt.xticks(rotation=\"vertical\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "#\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "#\n",
    "# mfccs = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=40)\n",
    "# print(mfccs.shape)\n",
    "# print(mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "import librosa\n",
    "\n",
    "def features_extractor():\n",
    "    #load the file (audio)\n",
    "    aud, smple_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "    #we extract mfcc\n",
    "    mfccs_f = librosa.feature.mfcc(y=aud, sr=smple_rate, n_mfcc=8732)\n",
    "    #in order to find out scaled feature, we do mean of transpose of value\n",
    "    mfccs_scaled_f = np.mean(mfccs_f.T, axis=0)\n",
    "    return mfccs_scaled_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "#Now we need to extract the featured from all the audio files, so we use tqdm\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# Now we iterate through every audio file and extract features\n",
    "# using Mel-Frequency Cepstral Coefficients (MFCC)\n",
    "extracted_features=[]\n",
    "for index_num,row in tqdm(metadata.iterrows()):\n",
    "    file_name = os.path.join(os.path.abspath('C:/Users/ashva/Projects/Audio Classifier/model/UrbanSound8K/audio'),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    final_class_labels=row[\"class\"]\n",
    "    data=features_extractor()\n",
    "    extracted_features.append([data,final_class_labels])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "# converting extracted_features to Pandas dataframe\n",
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "# Split the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())\n",
    "\n",
    "# Label Encoding -> Label Encoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder=LabelEncoder()\n",
    "y=to_categorical(label_encoder.fit_transform(y))\n",
    "\n",
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Audio Classification Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics\n",
    "# No of classes\n",
    "num_labels=y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "model: object=Sequential()\n",
    "#first layer\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#second layer\n",
    "model.add(Dense(2000))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#third layer\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#final layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy', 'Precision', 'Recall'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "# Training my model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./audio_classification.hdf5',\n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "duration = datetime.now() - start\n",
    "\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "# Check the Test Accuracy\n",
    "test_accuracy=model.evaluate(X_test,y_test,verbose=1)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "#model.predict_classes(X_test)\n",
    "predict_x=model.predict(X_test)\n",
    "classes_x=np.argmax(predict_x,axis=1)\n",
    "print(classes_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'audio_classification_model.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "93e0de3fc8884b1f2c82635fb4927f444ec48ba9f8a894b5f103101d00333d11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
